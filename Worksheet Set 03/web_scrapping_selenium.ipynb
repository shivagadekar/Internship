{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1f257d",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "### This task will be done in following steps:\n",
    "### 1. First get the webpage https://www.naukri.com/\n",
    "### 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "### 3. Then click the search button.\n",
    "### 4. Then scrape the data for the first 10 jobs results you get.\n",
    "### 5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dacd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException , NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ab2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0401af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')\n",
    "# time(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec4db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b726d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "locations.send_keys('Banglaore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aedcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME, 'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603cbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title1 = []\n",
    "job_location1 = []\n",
    "company_name1 = []\n",
    "experiance_required1 = []\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title1.append(title)\n",
    "    \n",
    "    \n",
    "location_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location1.append(location)\n",
    "    \n",
    "    \n",
    "company_tags = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name1.append(company)\n",
    "    \n",
    "experiance_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experiance_tags[0:10]:\n",
    "    experiance = i.text\n",
    "    experiance_required1.append(experiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20fbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Title\"] = job_title1\n",
    "df[\"Job_location\"] = job_location1\n",
    "df[\"Company_Name\"] = company_name1\n",
    "df[\"Experiance Required\"] = experiance_required1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e1eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Roles For Data Analyst in Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experiance Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Job_location, Company_Name, Experiance Required]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Job Roles For Data Analyst in Bangalore\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb757d6",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "### This task will be done in following steps:\n",
    "### 1. First get the webpage https://www.naukri.com/\n",
    "### 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "### 3. Then click the search button.\n",
    "### 4. Then scrape the data for the first 10 jobs results you get.\n",
    "### 5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f17ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b145b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')\n",
    "# time(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "843a7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation = driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3820b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "locations.send_keys('Banglaore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1568fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME, 'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f12910",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title2 = []\n",
    "job_location2 = []\n",
    "company_name2 = []\n",
    "experiance_required2 = []\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title2.append(title)\n",
    "    \n",
    "    \n",
    "location_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location2.append(location)\n",
    "    \n",
    "    \n",
    "company_tags = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name2.append(company)\n",
    "    \n",
    "experiance_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experiance_tags[0:10]:\n",
    "    experiance = i.text\n",
    "    experiance_required2.append(experiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Title\"] = job_title2\n",
    "df[\"Job_location\"] = job_location2\n",
    "df[\"Company_Name\"] = company_name2\n",
    "df[\"Experiance Required\"] = experiance_required2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "759bdcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Job Roles For Data Scientist in Bangalore \u001b[0;0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experiance Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mongodb Database Administrator, Maria DB or Ca...</td>\n",
       "      <td>Delhi / NCR, Hyderabad/Secunderabad, Pune, Che...</td>\n",
       "      <td>Mphasis</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi / NCR, New Delhi, Pune, Gurgaon/Gurugram...</td>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opportunity For Senior Data Scientist/ Busines...</td>\n",
       "      <td>Delhi / NCR, Gurgaon/Gurugram, Bangalore/Benga...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Exciting opportunity For model monitoring/mode...</td>\n",
       "      <td>Delhi / NCR, New Delhi, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>EXL</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>EXL</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Nagpur, Bangalore/Bengaluru</td>\n",
       "      <td>GlobalLogic</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>IBM</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Applied Intelligence - Data &amp; Insights -...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Mongodb Database Administrator, Maria DB or Ca...   \n",
       "1                   Analystics & Modeling Specialist   \n",
       "2                                     Data Scientist   \n",
       "3                              Senior Data Scientist   \n",
       "4  Opportunity For Senior Data Scientist/ Busines...   \n",
       "5  Exciting opportunity For model monitoring/mode...   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8                 Data Scientist: Advanced Analytics   \n",
       "9  ACN - Applied Intelligence - Data & Insights -...   \n",
       "\n",
       "                                        Job_location             Company_Name  \\\n",
       "0  Delhi / NCR, Hyderabad/Secunderabad, Pune, Che...                  Mphasis   \n",
       "1  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...                Accenture   \n",
       "2  Delhi / NCR, New Delhi, Pune, Gurgaon/Gurugram...            ZS Associates   \n",
       "3    Mumbai, New Delhi, Chennai, Bangalore/Bengaluru  Boston Consulting Group   \n",
       "4  Delhi / NCR, Gurgaon/Gurugram, Bangalore/Benga...                     PayU   \n",
       "5  Delhi / NCR, New Delhi, Gurgaon/Gurugram, Bang...                      EXL   \n",
       "6              Gurgaon/Gurugram, Bangalore/Bengaluru                      EXL   \n",
       "7                 Noida, Nagpur, Bangalore/Bengaluru              GlobalLogic   \n",
       "8                                   Gurgaon/Gurugram                      IBM   \n",
       "9                                   Gurgaon/Gurugram                Accenture   \n",
       "\n",
       "  Experiance Required  \n",
       "0            9-14 Yrs  \n",
       "1             6-8 Yrs  \n",
       "2             5-8 Yrs  \n",
       "3            5-10 Yrs  \n",
       "4             4-6 Yrs  \n",
       "5             4-7 Yrs  \n",
       "6             4-7 Yrs  \n",
       "7            8-10 Yrs  \n",
       "8             3-7 Yrs  \n",
       "9             4-6 Yrs  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\033[1m Job Roles For Data Scientist in Bangalore \\033[0;0m\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e7e10",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "### You have to use the location and salary filter.\n",
    "### You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "### You have to scrape the job-title, job-location, company name, experience required.\n",
    "### The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d41e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/')\n",
    "designation = driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "locations = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "locations.send_keys('Delhi/NCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d516922",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.CLASS_NAME, 'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e080a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_filter = driver.find_element(By.XPATH, '/html/body/div[1]/div[4]/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p')\n",
    "search_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "959b03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title3 = []\n",
    "job_location3 = []\n",
    "company_name3 = []\n",
    "experiance_required3 = []\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title = i.text\n",
    "    job_title3.append(title)\n",
    "    \n",
    "    \n",
    "location_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location = i.text\n",
    "    job_location3.append(location)\n",
    "    \n",
    "    \n",
    "company_tags = driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company = i.text\n",
    "    company_name3.append(company)\n",
    "    \n",
    "experiance_tags = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in experiance_tags[0:10]:\n",
    "    experiance = i.text\n",
    "    experiance_required3.append(experiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88ab871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Job Roles For Data Scientist in Bangalore \u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m Job Roles For Data Scientist in Bangalore \\033[0;0m\")\n",
    "df = pd.DataFrame()\n",
    "df[\"Title\"] = job_title3\n",
    "df[\"Job_location\"] = job_location3\n",
    "df[\"Company_Name\"] = company_name3\n",
    "df[\"Experiance Required\"] = experiance_required3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36433d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Job_location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experiance Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Module Lead - BIDW</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Mphasis</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist/AIML Engineer</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...</td>\n",
       "      <td>upGrad</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geospatial Data Engineer/Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Louis Dreyfus Commodities</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python Programming Language Data Science Pract...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Tata Consultancy Services (tcs)</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Nagpur, Bangalore/Bengaluru</td>\n",
       "      <td>GlobalLogic</td>\n",
       "      <td>8-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Scientific Computing (Proactive SO)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CGI</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Infosys</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Infosys</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                            Data Science Specialist   \n",
       "1                                 Module Lead - BIDW   \n",
       "2                       Data Scientist/AIML Engineer   \n",
       "3                 Geospatial Data Engineer/Scientist   \n",
       "4  Python Programming Language Data Science Pract...   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                Scientific Computing (Proactive SO)   \n",
       "8                                     Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        Job_location  \\\n",
       "0  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2  Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                             Hyderabad/Secunderabad   \n",
       "6                 Noida, Nagpur, Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                      Company_Name Experiance Required  \n",
       "0                        Accenture             2-4 Yrs  \n",
       "1                          Mphasis             5-8 Yrs  \n",
       "2                           upGrad             0-2 Yrs  \n",
       "3        Louis Dreyfus Commodities            5-10 Yrs  \n",
       "4                        Accenture             4-6 Yrs  \n",
       "5  Tata Consultancy Services (tcs)             4-6 Yrs  \n",
       "6                      GlobalLogic            8-10 Yrs  \n",
       "7                              CGI             3-7 Yrs  \n",
       "8                          Infosys             2-7 Yrs  \n",
       "9                          Infosys             2-7 Yrs  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d67e55",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation. You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c51ce0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3914a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/chrome_driver/chromedriver.exe')\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "salary = driver.find_element(By.XPATH, '//*[@id=\"ambitionbox-header\"]/nav/ul/li[3]/span')\n",
    "salary.click()\n",
    "driver.implicitly_wait(5)\n",
    "second_click = driver.find_element(By.LINK_TEXT, 'Browse salaries')\n",
    "second_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "206d5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = driver.find_element(By.XPATH, '//input[@id=\"jobProfileSearchbox\"]')\n",
    "search_text.send_keys('Data Scientist')\n",
    "driver.implicitly_wait(5)\n",
    "search_text_click = driver.find_element(By.XPATH, '//*[@id=\"salaries\"]/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p')\n",
    "search_text_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e09d11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = driver.find_elements(By.XPATH, '//div[@class=\"result-row\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99b01cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "companies = []\n",
    "for i in elements:\n",
    "    temp = i.find_elements(By.TAG_NAME, 'a')\n",
    "    for i in temp:\n",
    "        companies.append(i.text[:-22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e8ea126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiance = []\n",
    "for i in elements:\n",
    "    temp = i.find_elements(By.CLASS_NAME, 'sbold-list-header')\n",
    "    for i in temp:\n",
    "        experiance.append(i.text)\n",
    "        \n",
    "totaL_salary_record = []\n",
    "final_experiance = []\n",
    "for i in range(0, len(experiance)):\n",
    "    a = experiance[i].split(' (')\n",
    "    final_experiance.append(a[0])\n",
    "    totaL_salary_record.append(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57555b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_elements = driver.find_elements(By.XPATH, '//p[@class=\"averageCtc\"]')\n",
    "average_sal = []\n",
    "for i in avg_elements:\n",
    "    average_sal.append(i.text)\n",
    "average_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f6fe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_elements = driver.find_elements(By.XPATH, '//div[@class=\"salary-values\"]')\n",
    "min_max = []\n",
    "for i in min_max_elements:\n",
    "    min_max.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "246b948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mina = []\n",
    "maxa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3451b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in min_max:\n",
    "    a = i.split('\\n')\n",
    "    # print(a)\n",
    "    mina.append(a[0])\n",
    "    maxa.append(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "adecaddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Companies</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Experiance</th>\n",
       "      <th>Min Salary</th>\n",
       "      <th>Avg Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 24 salaries)</td>\n",
       "      <td>3-4 yrs experience</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 32.2L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 59 salaries)</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 19.8L</td>\n",
       "      <td>₹ 26.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 49 salaries)</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 16.4L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 35 salaries)</td>\n",
       "      <td>1-2 yrs experience</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 118 salaries)</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 15.5L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sigmoid Analytics</td>\n",
       "      <td>based on 10 salaries)</td>\n",
       "      <td>1 yr experience</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "      <td>₹ 19.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 70 salaries)</td>\n",
       "      <td>2-4 yrs experience</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 14.6L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Legato Health Technologies</td>\n",
       "      <td>based on 11 salaries)</td>\n",
       "      <td>4 yrs experience</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>based on 10 salaries)</td>\n",
       "      <td>4 yrs experience</td>\n",
       "      <td>₹ 12.0L</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 18.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tredence</td>\n",
       "      <td>based on 14 salaries)</td>\n",
       "      <td>3 yrs experience</td>\n",
       "      <td>₹ 8.8L</td>\n",
       "      <td>₹ 13.9L</td>\n",
       "      <td>₹ 17.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Companies                  Salary          Experiance  \\\n",
       "0                     Walmart   based on 24 salaries)  3-4 yrs experience   \n",
       "1                    Ab Inbev   based on 59 salaries)  2-4 yrs experience   \n",
       "2                       Optum   based on 49 salaries)  2-4 yrs experience   \n",
       "3                          ZS   based on 35 salaries)  1-2 yrs experience   \n",
       "4           Fractal Analytics  based on 118 salaries)  2-4 yrs experience   \n",
       "5           Sigmoid Analytics   based on 10 salaries)     1 yr experience   \n",
       "6             Tiger Analytics   based on 70 salaries)  2-4 yrs experience   \n",
       "7  Legato Health Technologies   based on 11 salaries)    4 yrs experience   \n",
       "8                        HSBC   based on 10 salaries)    4 yrs experience   \n",
       "9                    Tredence   based on 14 salaries)    3 yrs experience   \n",
       "\n",
       "  Min Salary Avg Salary Max Salary  \n",
       "0    ₹ 25.0L    ₹ 32.2L    ₹ 45.0L  \n",
       "1    ₹ 15.0L    ₹ 19.8L    ₹ 26.0L  \n",
       "2    ₹ 11.0L    ₹ 16.4L    ₹ 22.6L  \n",
       "3    ₹ 11.0L    ₹ 15.9L    ₹ 22.0L  \n",
       "4     ₹ 9.0L    ₹ 15.5L    ₹ 23.0L  \n",
       "5    ₹ 12.7L    ₹ 14.7L    ₹ 19.7L  \n",
       "6     ₹ 9.0L    ₹ 14.6L    ₹ 20.0L  \n",
       "7    ₹ 11.0L    ₹ 14.5L    ₹ 20.0L  \n",
       "8    ₹ 12.0L    ₹ 14.0L    ₹ 18.0L  \n",
       "9     ₹ 8.8L    ₹ 13.9L    ₹ 17.5L  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Companies'] = companies\n",
    "df[\"Salary\"] = totaL_salary_record\n",
    "df[\"Experiance\"] = final_experiance\n",
    "df[\"Min Salary\"] = mina\n",
    "df[\"Avg Salary\"] = average_sal\n",
    "df[\"Max Salary\"] = maxa\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802a0fc",
   "metadata": {},
   "source": [
    "# Q7 - Go to the link - https://www.myntra.com/shoes. Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "## And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException , NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5431f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/chrome_driver/chromedriver.exe')\n",
    "driver.get('https://www.myntra.com/shoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = driver.find_element(By.XPATH, '//*[@id=\"mountRoot\"]/div/div[1]/main/div/div[1]/section/div/div[5]/ul/li[2]')\n",
    "price.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = driver.find_element(By.XPATH, '//*[@id=\"mountRoot\"]/div/div[1]/main/div/div[1]/section/div/div[6]/ul/li[1]')\n",
    "color.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7869e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "brands=driver.find_elements(By.CLASS_NAME, 'product-brand')#scraping brands name by class name='_2B_pmu'\n",
    "for i in brands:\n",
    "    brand.append(i.text)#appending the text in Brand list\n",
    "#    \n",
    "desc=driver.find_elements(By.CLASS_NAME, 'product-product')#scraping description by class name = '_2mylT6'\n",
    "for i in desc:\n",
    "    description.append(i.text)#appending the description in list\n",
    "#\n",
    "prices=driver.find_elements(By.XPATH, '//div[@class=\"product-price\"]') # scraping the price from the xpath\n",
    "for i in prices:\n",
    "    price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nxt_button=driver.find_element(By.XPATH, '//*[@id=\"desktopSearchResults\"]/div[2]/section/div[2]/ul/li[12]/a')#scraping the list of buttons from the page\n",
    "nxt_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce01f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "brands=driver.find_elements(By.CLASS_NAME, 'product-brand')#scraping brands name by class name='_2B_pmu'\n",
    "for i in brands:\n",
    "    brand.append(i.text)#appending the text in Brand list\n",
    "#    \n",
    "desc=driver.find_elements(By.CLASS_NAME, 'product-product')#scraping description by class name = '_2mylT6'\n",
    "for i in desc:\n",
    "    description.append(i.text)#appending the description in list\n",
    "#\n",
    "prices=driver.find_elements(By.XPATH, '//div[@class=\"product-price\"]') # scraping the price from the xpath\n",
    "for i in prices:\n",
    "    price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Brand'] = brand\n",
    "df['Description'] = description\n",
    "df['Price | Actual Price | Discount'] = price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d373935",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/.\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException , NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/chrome_driver/chromedriver.exe')\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66744fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "product.send_keys('Laptop')\n",
    "product.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceffa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = driver.find_element(By.XPATH, '//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span')\n",
    "processor.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "rating = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52521955",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_name_element = driver.find_elements(By.XPATH, '//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "for laptop in laptop_name_element[:10]:\n",
    "    i = laptop.text\n",
    "    title.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a ratings box\n",
    "ratings_box = driver.find_elements(By.XPATH, '//div[@class=\"a-row a-size-small\"]/span')\n",
    "for i in range(0, 20):\n",
    "    ratings = ratings_box[i].get_attribute('aria-label')\n",
    "    if len(ratings) > 10:\n",
    "        if len(rating) == 10:\n",
    "            break\n",
    "        else:\n",
    "            rating.append(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_element = driver.find_elements(By.XPATH, '//span[@class=\"a-price-whole\"]')\n",
    "for _ in price_element[:10]:\n",
    "    i = _.text\n",
    "    price.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Title'] = title\n",
    "df['Rating'] = rating\n",
    "df['Price'] = price\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcd379",
   "metadata": {},
   "source": [
    "# Q4 - Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba05957",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "button1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//button[@class=\\\"_2KpZ6l _2doB4z\\\"]')))\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_3704LK')))\n",
    "search_bar.send_keys('sunglasses')\n",
    "search_bar.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60445eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names = []\n",
    "goggle_names = []\n",
    "goggle_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46481402",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a950ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]/span')\n",
    "next_page.click()\n",
    "\n",
    "\n",
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span')\n",
    "next_page.click()\n",
    "\n",
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10612373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Brand'] = brand_names[:100]\n",
    "df['Goggle Name'] = goggle_names[:100]\n",
    "df[\"Price\"] = goggle_price[:100]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8258adb",
   "metadata": {},
   "source": [
    "# Q6 - Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/')\n",
    "button1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//button[@class=\\\"_2KpZ6l _2doB4z\\\"]')))\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dddb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_3704LK')))\n",
    "search_bar.send_keys('sneakers')\n",
    "search_bar.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f06b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names = []\n",
    "goggle_names = []\n",
    "goggle_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeaa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span')\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa593c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]/span')\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da584e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_2WkVRV\\\"]')\n",
    "for i in brand_names_element[0:100]:\n",
    "    text = i.text\n",
    "    brand_names.append(text)\n",
    "    \n",
    "    \n",
    "goggle_names_element = driver.find_elements(By.XPATH, '//a[@class=\\\"IRpwTa\\\"]')\n",
    "for i in goggle_names_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_names.append(text)\n",
    "    \n",
    "    \n",
    "price_element = driver.find_elements(By.XPATH, '//div[@class=\\\"_30jeq3\\\"]')\n",
    "for i in price_element[0:100]:\n",
    "    text = i.text\n",
    "    goggle_price.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Brand'] = brand_names[:100]\n",
    "df['Product Description'] = goggle_names[:100]\n",
    "df[\"Price\"] = goggle_price[:100]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03bc91",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4117d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(\"C:/chrome_driver/chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/')\n",
    "button1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//button[@class=\\\"_2KpZ6l _2doB4z\\\"]')))\n",
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_3704LK')))\n",
    "search_bar.send_keys('iphone 11')\n",
    "search_bar.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b61bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '//a[@class=\"_1fQZEK\"]')))\n",
    "a = phone.get_attribute('href')\n",
    "driver.get(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[7]/div/a/div')\n",
    "all_review.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "review_title = []\n",
    "review_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e04e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title_element = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "for _ in review_title_element:\n",
    "    a = _.text\n",
    "    review_title.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64621b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_description_element = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]')\n",
    "for _ in review_description_element:\n",
    "    a = _.text\n",
    "    review_description.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_element = driver.find_elements(By.XPATH, '//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for _ in ratings_element:\n",
    "    a = _.text\n",
    "    rating.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in link:\n",
    "    \n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "        \n",
    "    review_title_element = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "    for _ in review_title_element:\n",
    "        a = _.text\n",
    "        review_title.append(a)\n",
    "        \n",
    "    review_description_element = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]')\n",
    "    for _ in review_description_element:\n",
    "        a = _.text\n",
    "        review_description.append(a)\n",
    "        \n",
    "    ratings_element = driver.find_elements(By.XPATH, '//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for _ in ratings_element:\n",
    "        a = _.text\n",
    "        rating.append(a)\n",
    "        \n",
    "        \n",
    "    links = driver.find_elements(By.XPATH, '//a[@class=\"ge-49M\"]')\n",
    "    for _ in links:\n",
    "        linka = _.get_attribute('href')\n",
    "        if linka not in link:\n",
    "            link.append(_.get_attribute('href'))\n",
    "            \n",
    "        if len(link) == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb84d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['rating'] = rating[:100]\n",
    "df['review_title'] = review_title[:100]\n",
    "df['review_description'] = review_description[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13571b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662424ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ce116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b38ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
