{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb59c4d",
   "metadata": {},
   "source": [
    "# <font color='red'>Importing Required Library</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df34684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7ae6f",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>1. Write a python program to display all the header tags from wikipedia.org.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dea375b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading \t Text\n",
      "h1 \t - > \t Main Page\n",
      "h1 \t - > \t Welcome to Wikipedia\n",
      "h2 \t - > \t From today's featured article\n",
      "h2 \t - > \t Did you know ...\n",
      "h2 \t - > \t In the news\n",
      "h2 \t - > \t On this day\n",
      "h2 \t - > \t Today's featured picture\n",
      "h2 \t - > \t Other areas of Wikipedia\n",
      "h2 \t - > \t Wikipedia's sister projects\n",
      "h2 \t - > \t Wikipedia languages\n",
      "h2 \t - > \t Navigation menu\n",
      "h3 \t - > \t Personal tools\n",
      "h3 \t - > \t Namespaces\n",
      "h3 \t - > \t Views\n",
      "h3 \t - > \t Search\n",
      "h3 \t - > \t Navigation\n",
      "h3 \t - > \t Contribute\n",
      "h3 \t - > \t Tools\n",
      "h3 \t - > \t Print/export\n",
      "h3 \t - > \t In other projects\n",
      "h3 \t - > \t Languages\n"
     ]
    }
   ],
   "source": [
    "def header_tags(link):  # Defining function\n",
    "    html = requests.get(link)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    # finding all heading with find all function\n",
    "    headings = soup.find_all(['h1', 'h2','h3','h4','h5'])\n",
    "    \n",
    "    print(f'Heading \\t Text')  # Table Headings\n",
    "    # Applying for loop to extract heading text from html\n",
    "    for heading in headings:\n",
    "        print(f\"{heading.name} \\t - > \\t {heading.get_text().strip()}\")\n",
    "        \n",
    "header_tags(\"https://en.wikipedia.org/wiki/Main_Page\")  # Running Function, that takes link as arguement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea675c",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ae6e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>1941</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>1931</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>1959</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Idi i smotri</td>\n",
       "      <td>1985</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>.Vertigo</td>\n",
       "      <td>1958</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                         Movie Name  Year Ratings\n",
       "0      1           The Shawshank Redemption  1994     9.2\n",
       "1      2                      The Godfather  1972     9.2\n",
       "2      3                    The Dark Knight  2008     9.0\n",
       "3      4              The Godfather Part II  1974     9.0\n",
       "4      5                       12 Angry Men  1957     8.9\n",
       "..   ...                                ...   ...     ...\n",
       "95    96                       Citizen Kane  1941     8.3\n",
       "96    97  M - Eine Stadt sucht einen Mörder  1931     8.3\n",
       "97    98                 North by Northwest  1959     8.3\n",
       "98    99                       Idi i smotri  1985     8.2\n",
       "99   100                           .Vertigo  1958     8.2\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imdb_top_100(url=\"https://www.imdb.com/chart/top/\"):  # Defining function\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    html.content\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    soup.prettify\n",
    "    \n",
    "    # Finding Movie Names\n",
    "    movie_names = soup.find_all('td', class_=\"titleColumn\")\n",
    "    movies = []\n",
    "    for movie in movie_names:\n",
    "        movie = movie.get_text().replace(\"\\n\", \"\")\n",
    "        movies.append(movie.strip(\" \"))\n",
    "        \n",
    "    #Movie Ratings\n",
    "    all_ratings = soup.find_all('td', class_=\"ratingColumn imdbRating\")\n",
    "    ratings = []\n",
    "    for rating in all_ratings:\n",
    "        rating = rating.get_text().replace(\"\\n\", \"\")\n",
    "        ratings.append(rating)\n",
    "        \n",
    "    # Creating data frame and extracting name, year, number\n",
    "    df = pd.DataFrame()\n",
    "    df['movies'] = movies\n",
    "    df['ratings'] = ratings\n",
    "\n",
    "    # Removing additional Spaces\n",
    "    new_movies = []\n",
    "    for i in df[\"movies\"]:\n",
    "        a = i.replace(\"      \", \"\")\n",
    "        new_movies.append(a)\n",
    "    # print(len(new_movies))\n",
    "\n",
    "    # Adding New Data Frame to DF\n",
    "\n",
    "    df[\"new\"] = new_movies\n",
    "    \n",
    "    \n",
    "    # Seperating Releasing Year\n",
    "    release_year = []\n",
    "    for i in range(0, len(df)):\n",
    "        a_string = df[\"movies\"][i]\n",
    "        result = re.search(r\"\\(([A-Za-z0-9_]+)\\)\", a_string)\n",
    "        release_year.append(result.group(1))\n",
    "    # print(release_year)\n",
    "\n",
    "\n",
    "    \n",
    "    # Adding releasing year to DF\n",
    "    df[\"Release Year\"] = release_year  # Seperating release year\n",
    "    df.head()\n",
    "    \n",
    "    \n",
    "    # Deleting Existing Name from Data Frame\n",
    "    # New_Movie_name added in file\n",
    "    movie_name = []\n",
    "    for i in range(0, len(df[\"new\"])):\n",
    "        in_str = str(i)\n",
    "        len_of_st = len(in_str) + 1\n",
    "        a = df[\"new\"][i][len_of_st:-6]\n",
    "        movie_name.append(a)\n",
    "    df[\"Movie Name\"] = movie_name\n",
    "    \n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    # new_df.style.hide_index()\n",
    "    new_df[\"Rank\"] = [i for i in range(1, 251)]\n",
    "    new_df[\"Movie Name\"] = movie_name\n",
    "    new_df[\"Year\"] = release_year\n",
    "    new_df[\"Ratings\"] = ratings\n",
    "    return new_df.head(100)\n",
    "\n",
    "imdb_top_100(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a7a7e",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57687a",
   "metadata": {},
   "source": [
    "### NOTE: As we are using, same website to scrap data. After studying structure of \"Top 100 Indian movies\", i can say that, same function (imdb_top_100) can be applied to extract top 100 Indian movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26007891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Top 100 Indian Movies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>2022</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>2003</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>2021</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Nayakan</td>\n",
       "      <td>1987</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Golmaal</td>\n",
       "      <td>1979</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>2012</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>The Legend of Bhagat Singh</td>\n",
       "      <td>2002</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Virumandi</td>\n",
       "      <td>2004</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>2017</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>.Angoor</td>\n",
       "      <td>1982</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                   Movie Name  Year Ratings\n",
       "0      1   Rocketry: The Nambi Effect  2022     8.5\n",
       "1      2                   Anbe Sivam  2003     8.4\n",
       "2      3                     Jai Bhim  2021     8.4\n",
       "3      4                      Nayakan  1987     8.4\n",
       "4      5                      Golmaal  1979     8.4\n",
       "..   ...                          ...   ...     ...\n",
       "95    96                  Ustad Hotel  2012     8.0\n",
       "96    97   The Legend of Bhagat Singh  2002     8.0\n",
       "97    98                    Virumandi  2004     8.0\n",
       "98    99  Baahubali 2: The Conclusion  2017     8.0\n",
       "99   100                      .Angoor  1982     8.0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\033[1m Top 100 Indian Movies\")\n",
    "imdb_top_100(url=\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed7350",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>4. Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)</font>\n",
    "## Link: https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bcdd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Presidents     Life Span  \\\n",
      "0           Shri Ram Nath Kovind   birth - 1945   \n",
      "1          Shri Pranab Mukherjee      1935-2020   \n",
      "2   Smt Pratibha Devisingh Patil   birth - 1934   \n",
      "3         DR. A.P.J. Abdul Kalam      1931-2015   \n",
      "4           Shri K. R. Narayanan    1920 - 2005   \n",
      "5        Dr Shankar Dayal Sharma      1918-1999   \n",
      "6            Shri R Venkataraman      1910-2009   \n",
      "7               Giani Zail Singh      1916-1994   \n",
      "8      Shri Neelam Sanjiva Reddy      1913-1996   \n",
      "9       Dr. Fakhruddin Ali Ahmed      1905-1977   \n",
      "10  Shri Varahagiri Venkata Giri      1894-1980   \n",
      "11              Dr. Zakir Husain      1897-1969   \n",
      "12  Dr. Sarvepalli Radhakrishnan      1888-1975   \n",
      "13           Dr. Rajendra Prasad      1884-1963   \n",
      "\n",
      "                                          Term Served  \n",
      "0                      25 July, 2017 to 25 July, 2022  \n",
      "1                      25 July, 2012 to 25 July, 2017  \n",
      "2                      25 July, 2007 to 25 July, 2012  \n",
      "3                      25 July, 2002 to 25 July, 2007  \n",
      "4                      25 July, 1997 to 25 July, 2002  \n",
      "5                      25 July, 1992 to 25 July, 1997  \n",
      "6                      25 July, 1987 to 25 July, 1992  \n",
      "7                      25 July, 1982 to 25 July, 1987  \n",
      "8                      25 July, 1977 to 25 July, 1982  \n",
      "9                 24 August, 1974 to 11 February, 197  \n",
      "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
      "11                         13 May, 1967 to 3 May, 196  \n",
      "12                        13 May, 1962 to 13 May, 196  \n",
      "13                    26 January, 1950 to 13 May, 196  \n"
     ]
    }
   ],
   "source": [
    "def presidents_info(url):\n",
    "    page = requests.get(url)\n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup.prettify\n",
    "    president_name = soup.find_all('ul', class_=\"listing cf\")\n",
    "\n",
    "    lis = []\n",
    "    new_list = []\n",
    "    for ul in president_name:\n",
    "        for li in ul.findAll('li'):\n",
    "            if li.find('ul'):\n",
    "                break\n",
    "            lis.append(li)\n",
    "\n",
    "        for li in lis:\n",
    "            # print(li.text.encode(\"utf-8\"))\n",
    "            new_list.append(li.text.encode(\"utf-8\"))\n",
    "    # Extracting Names\n",
    "    presidents = []\n",
    "    for i in range(0, len(new_list)):\n",
    "        a = str(new_list[i]).replace(\"\\\\n\", \"\")\n",
    "        leb_ = a.find(\"(\")\n",
    "        # print(a[2:leb_])\n",
    "        presidents.append(a[2:leb_])\n",
    "    # print(presidents)\n",
    "    \n",
    "    year = []\n",
    "    for i in range(0, len(new_list)):\n",
    "        a = str(new_list[i]).replace(\"\\\\n\", \"\")\n",
    "        start_ = a.find(\"(\")\n",
    "        end_ = a.find(\")\")\n",
    "        # print(a[start_:end_])\n",
    "        year.append(a[start_+1:end_])\n",
    "    # print(year)\n",
    "    \n",
    "    # Extracting Terms from page\n",
    "    term = []\n",
    "    for para in soup.find_all(\"p\"):\n",
    "        term.append(para.get_text())\n",
    "    final_term = []\n",
    "    for i in range(0, len(term)):\n",
    "        if term[i][:7] == \"Term of\":\n",
    "            # print(\"True\")\n",
    "            slicing = term[i][16:-1]\n",
    "            final_term.append(slicing)    \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Presidents\"] = presidents\n",
    "    df[\"Life Span\"] = year\n",
    "    df['Term Served'] = final_term\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\" \n",
    "\n",
    "presidents_info(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e1cfb",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>5. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:</font>\n",
    "### a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "### b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "### c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d98240",
   "metadata": {},
   "source": [
    "## <font color='green'>a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce4cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "Men's ODI Team Rankings \u001b[1m\n",
      "   Rank    Team Names Total Points Total Matchs Rating\n",
      "0     1       England        3,226           27    119\n",
      "1     2   New Zealand        2,508           22    114\n",
      "2     3         India        3,447           31    111\n",
      "3     4      Pakistan        2,354           22    107\n",
      "4     5     Australia        3,071           29    106\n",
      "5     6  South Africa        2,111           21    101\n",
      "6     7    Bangladesh        2,753           30     92\n",
      "7     8     Sri Lanka        2,658           29     92\n",
      "8     9   West Indies        2,902           41     71\n",
      "9    10   Afghanistan        1,238           18     69\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/test\"\n",
    "def top_10_teams(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    # Top 10 Team Names\n",
    "    team_name = soup.findAll(\"span\", {'class': 'u-hide-phablet'})\n",
    "    team_name_df = []\n",
    "    for name in team_name:\n",
    "        # print(name.get_text())\n",
    "        team_name_df.append(name.get_text())\n",
    "\n",
    "    # Match score and Points of last 9 teams\n",
    "    total_match_df = []\n",
    "    total_point_df = []\n",
    "    matches_and_points_df = []\n",
    "    \n",
    "    # Extracting total matches and points for first row, because of different class name\n",
    "    matches_and_points_df = []\n",
    "    matches_and_points_df.clear()\n",
    "    first_ = soup.findAll(\"td\", {'class': 'rankings-block__banner--matches'})\n",
    "    second_ = soup.findAll(\"td\", {'class': 'rankings-block__banner--points'})\n",
    "\n",
    "    for _ in first_:\n",
    "        matches_and_points_df.append(_.get_text())\n",
    "\n",
    "    for _ in second_:\n",
    "        matches_and_points_df.append(_.get_text())\n",
    "\n",
    "    # print(total_match_df)\n",
    "    # print(total_point_df)\n",
    "    \n",
    "    \n",
    "    matches = soup.findAll(\"td\", {'class': 'table-body__cell u-center-text'})\n",
    "\n",
    "    for match in matches:\n",
    "            # print(match.get_text())\n",
    "        matches_and_points_df.append(match.get_text())\n",
    "    res = matches_and_points_df[::2] + matches_and_points_df[1::2]\n",
    "    total_match_df = res[(len(matches_and_points_df)//2):]\n",
    "    total_point_df = res[:(len(matches_and_points_df)//2)]\n",
    "    rank = [i for i in range(1, len(total_match_df)+1)]\n",
    "    \n",
    "    \n",
    "    # Raking Ranking\n",
    "    # taking rating of first column\n",
    "    rating = []\n",
    "    rating_first = soup.findAll(\"td\", {'class': 'rankings-block__banner--rating u-text-right'})\n",
    "    rating_rest = soup.findAll(\"td\", {'class': 'table-body__cell u-text-right rating'})\n",
    "\n",
    "    for _ in rating_first:\n",
    "        pre_processor = _.get_text().strip()\n",
    "        print(pre_processor)\n",
    "        rating.append(pre_processor)\n",
    "    for _ in rating_rest:\n",
    "        rating.append(_.get_text())\n",
    "        #if len(rating) == 10:\n",
    "            # break\n",
    "    # print(rating[:10])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extracting Type of Data\n",
    "    text = soup.findAll(\"h4\")\n",
    "    for _ in text:\n",
    "        print(_.get_text(), '\\033[1m')\n",
    "    # creating data frame\n",
    "    df = pd.DataFrame()\n",
    "    df['Rank'] = rank\n",
    "    df[\"Team Names\"] = team_name_df\n",
    "    df[\"Total Points\"] = total_match_df\n",
    "    df[\"Total Matchs\"] = total_point_df\n",
    "    # print(len(total_match_df), \"and\", len(rating))\n",
    "    df[\"Rating\"] = rating\n",
    "    print(df.head(10))\n",
    "\n",
    "top_10_teams(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3eb09d",
   "metadata": {},
   "source": [
    "## <font color='green'>b) Top 10 ODI Batsmen along with the records of their team and rating.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108b74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men's ODI Batting Rankings \u001b[1m\n",
      "   Rank            Player Name Team Rating                    Career Best\n",
      "0     1             Babar Azam  PAK    890  898 v West Indies, 10/06/2022\n",
      "1     2  Rassie van der Dussen   SA    789      796 v England, 19/07/2022\n",
      "2     3        Quinton de Kock   SA    784    813 v Sri Lanka, 10/03/2019\n",
      "3     4            Imam-ul-Haq  PAK    779  815 v West Indies, 12/06/2022\n",
      "4     5            Virat Kohli  IND    744      911 v England, 12/07/2018\n",
      "5     6           Rohit Sharma  IND    740    885 v Sri Lanka, 06/07/2019\n",
      "6     7         Jonny Bairstow  ENG    732        796 v India, 26/03/2021\n",
      "7     8           David Warner  AUS    725     880 v Pakistan, 26/01/2017\n",
      "8     9            Ross Taylor   NZ    701   841 v Bangladesh, 05/06/2019\n",
      "9    10            Steve Smith  AUS    697     752 v Pakistan, 22/01/2017\n"
     ]
    }
   ],
   "source": [
    "def top_10_players(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "# Creating empty lists to append data\n",
    "    players_name_df = []\n",
    "    player_team_df = []\n",
    "    player_rating_df = []\n",
    "    careers_rating_df = []\n",
    "    \n",
    "    \n",
    "# Accessing only top player\n",
    "    # Top Player Name\n",
    "    top_player_name = soup.findAll(\"div\", {'class': 'rankings-block__banner--name-large'})\n",
    "    for name in top_player_name:\n",
    "        # print(name.get_text())\n",
    "        players_name_df.append(name.get_text())\n",
    "\n",
    "\n",
    "    # Top Player team\n",
    "    top_player_team = soup.findAll(\"div\", {'class': 'rankings-block__banner--nationality'})\n",
    "    for name in top_player_team:\n",
    "        a = name.get_text().strip()\n",
    "        # print(name.get_text())\n",
    "        player_team_df.append(a)    \n",
    "\n",
    "\n",
    "\n",
    "    # Top Player rating\n",
    "    top_player_rating = soup.findAll(\"div\", {'class': 'rankings-block__banner--rating'})\n",
    "    for name in top_player_rating:\n",
    "        a = name.get_text().strip()\n",
    "        # print(name.get_text())\n",
    "        player_rating_df.append(a)\n",
    "\n",
    "\n",
    "\n",
    "    # Top Player career Best\n",
    "    top_player_career_best = soup.findAll(\"span\", {'class': 'rankings-block__career-best-text'})\n",
    "    for name in top_player_career_best:\n",
    "        a = name.get_text().strip()\n",
    "        # print(name.get_text())\n",
    "        careers_rating_df.append(a)    \n",
    "\n",
    "    # print(careers_rating_df)\n",
    "    \n",
    "    \n",
    "\n",
    "# Accessing Rest of the players\n",
    "\n",
    "    # Rest of player Names\n",
    "    rest_player_name = soup.findAll(\"td\", {'class': 'table-body__cell rankings-table__name name'})\n",
    "    for name in rest_player_name:\n",
    "        a = name.get_text().strip()\n",
    "        # print(name.get_text())\n",
    "        players_name_df.append(a)\n",
    "\n",
    "\n",
    "    # Rest of player Teams\n",
    "    rest_player_team = soup.findAll(\"span\", {'class': 'table-body__logo-text'})\n",
    "    for name in rest_player_team:\n",
    "        # print(name.get_text())\n",
    "        player_team_df.append(name.get_text())\n",
    "\n",
    "\n",
    "    # Rest of player rating\n",
    "    rest_player_rating = soup.findAll(\"td\", {'class': 'table-body__cell rating'})\n",
    "    for name in rest_player_rating:\n",
    "        # print(name.get_text())\n",
    "        player_rating_df.append(name.get_text())\n",
    "\n",
    "    # Rest of player career Best\n",
    "    rest_player_career = soup.findAll(\"td\", {'class': 'table-body__cell u-text-right u-hide-phablet'})\n",
    "    for name in rest_player_career:\n",
    "        a = name.get_text().strip()\n",
    "        # print(name.get_text())\n",
    "        careers_rating_df.append(a)\n",
    "        \n",
    "    # Rank\n",
    "    rank = [i for i in range(1, len(players_name_df)+1)]\n",
    "    \n",
    "    text = soup.findAll(\"h4\")\n",
    "    for _ in text:\n",
    "        print(_.get_text(), '\\033[1m')\n",
    "\n",
    "    players_df = pd.DataFrame() \n",
    "    players_df[\"Rank\"] = rank    \n",
    "    players_df[\"Player Name\"] = players_name_df\n",
    "    players_df[\"Team\"] = player_team_df\n",
    "    players_df[\"Rating\"] = player_rating_df\n",
    "    players_df[\"Career Best\"] = careers_rating_df\n",
    "    print(players_df.head(10))\n",
    "    \n",
    "\n",
    "top_10_players(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70661e",
   "metadata": {},
   "source": [
    "## <font color='green'>c) Top 10 ODI bowlers along with the records of their team and rating.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1571f",
   "metadata": {},
   "source": [
    "### Note: As we are extracting top 10 bowlers, so after studing html, we can say that same \"top_10_players\" can be used to extract \"Top 10 ODI bowlers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805f8b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men's ODI Bowling Rankings \u001b[1m\n",
      "   Rank       Player Name Team Rating                    Career Best\n",
      "0     1       Trent Boult   NZ    775    775 v Australia, 11/09/2022\n",
      "1     2    Josh Hazlewood  AUS    718      733 v England, 26/01/2018\n",
      "2     3  Mujeeb Ur Rahman  AFG    676      712 v Ireland, 24/01/2021\n",
      "3     4    Jasprit Bumrah  IND    662  841 v West Indies, 01/11/2018\n",
      "4     5    Shaheen Afridi  PAK    661  688 v West Indies, 10/06/2022\n",
      "5     6     Mohammad Nabi  AFG    657     657 v Zimbabwe, 09/06/2022\n",
      "6     7      Mehedi Hasan  BAN    655    725 v Sri Lanka, 25/05/2021\n",
      "7     8        Matt Henry   NZ    654   691 v Bangladesh, 26/03/2021\n",
      "8     9    Mitchell Starc  AUS    653  783 v New Zealand, 29/03/2015\n",
      "9    10       Rashid Khan  AFG    651     806 v Pakistan, 21/09/2018\n"
     ]
    }
   ],
   "source": [
    "top_10_players(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7a784",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>6. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:</font>\n",
    "### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "### b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7970fa0",
   "metadata": {},
   "source": [
    "## <font color='green'>a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7116783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "Women's ODI Rankings \u001b[1m\n",
      "   Rank    Team Names Total Points Total Matchs Rating\n",
      "0     1     Australia        4,837           29    167\n",
      "1     2  South Africa        4,157           35    119\n",
      "2     3       England        4,205           36    117\n",
      "3     4         India        3,732           35    107\n",
      "4     5   New Zealand        3,302           33    100\n",
      "5     6   West Indies        2,864           32     90\n",
      "6     7    Bangladesh          930           12     78\n",
      "7     8      Pakistan        1,962           30     65\n",
      "8     9       Ireland          516           11     47\n",
      "9    10     Sri Lanka          495           11     45\n"
     ]
    }
   ],
   "source": [
    "top_10_teams(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cd4dc",
   "metadata": {},
   "source": [
    "## <font color='green'>b) Top 10 women’s ODI Batting players along with the records of their team and rating.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb83c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's ODI Batting Rankings \u001b[1m\n",
      "   Rank          Player Name Team Rating                     Career Best\n",
      "0     1         Alyssa Healy  AUS    785       785 v England, 03/04/2022\n",
      "1     2          Beth Mooney  AUS    749       748 v England, 03/04/2022\n",
      "2     3       Natalie Sciver  ENG    740  755 v South Africa, 15/07/2022\n",
      "3     4      Laura Wolvaardt   SA    732     741 v Australia, 22/03/2022\n",
      "4     5          Meg Lanning  AUS    710   834 v New Zealand, 24/02/2016\n",
      "5     6       Rachael Haynes  AUS    701   713 v West Indies, 15/03/2022\n",
      "6     7      Smriti Mandhana  IND    698       797 v England, 28/02/2019\n",
      "7     8    Amy Satterthwaite   NZ    681     756 v Australia, 02/03/2017\n",
      "8     9     Harmanpreet Kaur  IND    662  679 v South Africa, 28/11/2014\n",
      "9    10  Chamari Athapaththu   SL    655  691 v South Africa, 14/02/2019\n"
     ]
    }
   ],
   "source": [
    "top_10_players(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2f1d9",
   "metadata": {},
   "source": [
    "## <font color='green'>c) Top 10 women’s ODI all-rounder along with the records of their team and rating.¶</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3cdb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's ODI All-Rounder Rankings \u001b[1m\n",
      "   Rank        Player Name Team Rating                     Career Best\n",
      "0     1       Ellyse Perry  AUS    374   548 v West Indies, 11/09/2019\n",
      "1     2     Natalie Sciver  ENG    372  395 v South Africa, 11/07/2022\n",
      "2     3     Marizanne Kapp   SA    349   419 v West Indies, 10/09/2021\n",
      "3     4    Hayley Matthews   WI    339         365 v India, 12/03/2022\n",
      "4     5        Amelia Kerr   NZ    336  339 v South Africa, 17/03/2022\n",
      "5     6      Deepti Sharma  IND    271  397 v South Africa, 09/10/2019\n",
      "6     7   Ashleigh Gardner  AUS    270   279 v West Indies, 30/03/2022\n",
      "7     8      Jess Jonassen  AUS    246   308 v West Indies, 11/09/2019\n",
      "8     9     Jhulan Goswami  IND    219     308 v Australia, 02/02/2016\n",
      "9    10  Sophie Ecclestone  ENG    217  217 v South Africa, 31/03/2022\n"
     ]
    }
   ],
   "source": [
    "top_10_players(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b88cde",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>7. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world</font>\n",
    "####  i) Headline\n",
    "#### ii) Time\n",
    "#### iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27dba96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Time                                               News  \\\n",
      "0            4 Min Ago  Market rout has muni bonds looking attractive....   \n",
      "1           26 Min Ago    These are the 6 best business books of the year   \n",
      "2           26 Min Ago  Meet the most powerful Hollywood player you mi...   \n",
      "3           26 Min Ago  Pumpkin spice latte popularity comes down to '...   \n",
      "4           26 Min Ago  Why Mark Cuban says he’s not ready to retire a...   \n",
      "5          7 Hours Ago  Italy poised for hard-right leader as country ...   \n",
      "6         19 Hours Ago  3 rules for a successful open relationship, ac...   \n",
      "7         21 Hours Ago  Biden administration awards $1.5 billion to fi...   \n",
      "8         21 Hours Ago  Top 10 cities with the best pizzerias worldwid...   \n",
      "9         22 Hours Ago  Black Girls in Trader Joe’s creator shares her...   \n",
      "10        23 Hours Ago  Want to raise strong, resilient kids? Create '...   \n",
      "11        23 Hours Ago  Why the airline climate change plan is trailin...   \n",
      "12        23 Hours Ago  'Queer Eye's Karamo Brown on the morning routi...   \n",
      "13        24 Hours Ago  These 7 states have the least air pollution in...   \n",
      "14  September 24, 2022  New York is now No. 1 port in a tipping point ...   \n",
      "15  September 24, 2022  Analysts have 'high conviction' that these sto...   \n",
      "16  September 24, 2022  Feared stock market bottom retest is now underway   \n",
      "17  September 24, 2022  The No. 1 best city to retire isn't in Florida...   \n",
      "18  September 24, 2022  The perils and promise of quantum computing ar...   \n",
      "19  September 24, 2022  Everything parents need to know about student ...   \n",
      "20  September 24, 2022  Check in, smoke up and tune out: Cannabis-frie...   \n",
      "21  September 24, 2022  Convertibles drive into the sunset as automake...   \n",
      "22  September 24, 2022  Britain's lurch toward 'Reaganomics' gets a th...   \n",
      "23  September 23, 2022  Here's our plan for Monday after another painf...   \n",
      "24  September 23, 2022     What to watch in the markets in the week ahead   \n",
      "25  September 23, 2022  Pro Picks: Watch all of Friday's big stock cal...   \n",
      "26  September 23, 2022  13 careers where over 50% of workers are happy...   \n",
      "27  September 23, 2022  New York AG wrongly said Yankees game on Apple...   \n",
      "28  September 23, 2022  Tech stocks just had the worst two-week stretc...   \n",
      "29  September 23, 2022  Takeaways from Jim Cramer's interviews with th...   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2022/09/25/market-rout-ha...  \n",
      "1   https://www.cnbc.com/2022/09/25/the-6-best-bus...  \n",
      "2   https://www.cnbc.com/2022/09/25/bryan-lourd-ho...  \n",
      "3   https://www.cnbc.com/2022/09/25/what-pumpkin-s...  \n",
      "4   https://www.cnbc.com/2022/09/25/mark-cuban-say...  \n",
      "5   https://www.cnbc.com/2022/09/25/italy-poised-f...  \n",
      "6   https://www.cnbc.com/2022/09/24/three-rules-fo...  \n",
      "7   https://www.cnbc.com/2022/09/24/biden-administ...  \n",
      "8   https://www.cnbc.com/2022/09/24/top-10-cities-...  \n",
      "9   https://www.cnbc.com/2022/09/24/easy-meals-for...  \n",
      "10  https://www.cnbc.com/2022/09/24/how-to-raise-r...  \n",
      "11  https://www.cnbc.com/2022/09/24/how-airlines-p...  \n",
      "12  https://www.cnbc.com/2022/09/24/queer-eyes-kar...  \n",
      "13  https://www.cnbc.com/2022/09/24/vermont-new-me...  \n",
      "14  https://www.cnbc.com/2022/09/24/new-york-now-n...  \n",
      "15  https://www.cnbc.com/2022/09/24/analysts-name-...  \n",
      "16  https://www.cnbc.com/2022/09/24/feared-stock-m...  \n",
      "17  https://www.cnbc.com/2022/09/24/wallethub-top-...  \n",
      "18  https://www.cnbc.com/2022/09/24/quantum-invest...  \n",
      "19  https://www.cnbc.com/2022/09/24/what-parent-pl...  \n",
      "20  https://www.cnbc.com/2022/09/24/check-in-smoke...  \n",
      "21  https://www.cnbc.com/2022/09/24/convertible-sa...  \n",
      "22  https://www.cnbc.com/2022/09/24/liz-truss-brit...  \n",
      "23  https://www.cnbc.com/2022/09/23/it-was-another...  \n",
      "24  https://www.cnbc.com/2022/09/23/stocks-could-c...  \n",
      "25  https://www.cnbc.com/2022/09/23/pro-picks-watc...  \n",
      "26  https://www.cnbc.com/2022/09/23/careers-where-...  \n",
      "27  https://www.cnbc.com/2022/09/23/new-york-ag-wr...  \n",
      "28  https://www.cnbc.com/2022/09/23/tech-stocks-wo...  \n",
      "29  https://www.cnbc.com/2022/09/23/jim-cramer-sat...  \n"
     ]
    }
   ],
   "source": [
    "def top_news(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    # Extracting Time\n",
    "    time_df = []\n",
    "    time_list = soup.findAll(\"time\", {'class': 'LatestNews-timestamp'})\n",
    "    for time in time_list:\n",
    "        # print(time.get_text())\n",
    "        time_df.append(time.get_text())\n",
    "        \n",
    "    # Extracting Heading\n",
    "    heading_df = []\n",
    "    headings = soup.findAll(\"a\", {'class': 'LatestNews-headline'})\n",
    "    for heading in headings:\n",
    "        # print(heading.get_text())\n",
    "        heading_df.append(heading.get_text())\n",
    "        \n",
    "    # Extracting Links\n",
    "    links_df = []\n",
    "    news_link = soup.findAll(\"a\", {'class': 'LatestNews-headline'})\n",
    "    for link in news_link:\n",
    "        # print(link.get('href'))\n",
    "        links_df.append(link.get('href'))\n",
    "        \n",
    "    \n",
    "    # adding files to Df\n",
    "    news_df = pd.DataFrame()\n",
    "    news_df[\"Time\"] = time_df\n",
    "    news_df[\"News\"] = heading_df\n",
    "    news_df[\"Link\"] = links_df\n",
    "    print(news_df)\n",
    "\n",
    "top_news(\"https://www.cnbc.com/world/?region=world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5040f",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>8. Write a python program to scrape the details of most downloaded articles from AI in last 90 days. </font>\n",
    "## Link: https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "### Scrape below mentioned details :\n",
    "#### i) Paper Title\n",
    "#### ii) Authors\n",
    "#### iii) Published Date\n",
    "#### iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60064e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1                           Making sense of raw input   \n",
      "2   Law and logic: A review from an argumentation ...   \n",
      "3              Creativity and artificial intelligence   \n",
      "4   Artificial cognition for social human–robot in...   \n",
      "5   Explanation in artificial intelligence: Insigh...   \n",
      "6                       Making sense of sensory input   \n",
      "7   Conflict-based search for optimal multi-agent ...   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...   \n",
      "9   The Hanabi challenge: A new frontier for AI re...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11           Argumentation in artificial intelligence   \n",
      "12  Algorithms for computing strategies in two-pla...   \n",
      "13      Multiple object tracking: A literature review   \n",
      "14  Selection of relevant features and examples in...   \n",
      "15  A survey of inverse reinforcement learning: Ch...   \n",
      "16  Explaining individual predictions when feature...   \n",
      "17  A review of possible effects of cognitive bias...   \n",
      "18  Integrating social power into the decision-mak...   \n",
      "19  “That's (not) the output I expected!” On the r...   \n",
      "20  Explaining black-box classifiers using post-ho...   \n",
      "21  Algorithm runtime prediction: Methods & evalua...   \n",
      "22              Wrappers for feature subset selection   \n",
      "23  Commonsense visual sensemaking for autonomous ...   \n",
      "24         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                          Author Name  Published Date  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "3                                 Boden, Margaret A.      August 1998   \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "5                                        Miller, Tim    February 2019   \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "22                      Kohavi, Ron, John, George H.    December 1997   \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "24                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                           Paper Link  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "def most_downloaded_articles(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    # Extracting Paper Title\n",
    "    paper_title_df = []\n",
    "    titles = soup.findAll(\"h2\", {'class': 'sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR'})\n",
    "    for title in titles:\n",
    "        # print(title.get_text())\n",
    "        paper_title_df.append(title.get_text())\n",
    "        \n",
    "        \n",
    "    # Extracting Author Names\n",
    "    author_name_df = []\n",
    "    authors = soup.findAll(\"span\", {'class': 'sc-1w3fpd7-0 pgLAT'})\n",
    "    for author in authors:\n",
    "        # print(author.get_text())\n",
    "        author_name_df.append(author.get_text())\n",
    "        \n",
    "        \n",
    "    # Extracting Published Date\n",
    "    published_date_df = []\n",
    "    published = soup.findAll(\"span\", {'class': 'sc-1thf9ly-2 bKddwo'})\n",
    "    for date in published:\n",
    "        # print(date.get_text())\n",
    "        published_date_df.append(date.get_text())\n",
    "        \n",
    "    # Extracting Links\n",
    "    paper_links_df = []\n",
    "    paper_links = soup.findAll(\"a\", {'class': 'sc-5smygv-0 nrDZj'})\n",
    "    for link in paper_links:\n",
    "        # print(link.get('href'))\n",
    "        paper_links_df.append(link.get('href'))\n",
    "        \n",
    "               \n",
    "    papers = pd.DataFrame()\n",
    "    papers[\"Paper Title\"] = paper_title_df\n",
    "    papers[\"Author Name\"] = author_name_df\n",
    "    papers[\"Published Date\"] = published_date_df\n",
    "    papers[\"Paper Link\"] = paper_links_df\n",
    "\n",
    "    print(papers)\n",
    "        \n",
    "    \n",
    "most_downloaded_articles('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878aa4c",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>9) Write a python program to scrape mentioned details from dineout.co.in</font>\n",
    "#### i) Restaurant name\n",
    "#### ii) Cuisine\n",
    "#### iii) Location\n",
    "#### iv) Ratings\n",
    "#### v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb14565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant name                        Cuisine  \\\n",
      "0                   Castle Barbeque          Chinese, North Indian   \n",
      "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
      "2                   Castle Barbeque          Chinese, North Indian   \n",
      "3                        Cafe Knosh           Italian, Continental   \n",
      "4              The Barbeque Company          North Indian, Chinese   \n",
      "5                       India Grill          North Indian, Italian   \n",
      "6                    Delhi Barbeque                   North Indian   \n",
      "7  The Monarch - Bar Be Que Village                   North Indian   \n",
      "8                 Indian Grill Room          North Indian, Mughlai   \n",
      "\n",
      "                                            Location Ratings  \\\n",
      "0                     Connaught Place, Central Delhi     4.1   \n",
      "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
      "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
      "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
      "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "\n",
      "                                          Image Urls  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "def restaurant_details(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    # Extracting Restaurant name\n",
    "    hotels_df = []\n",
    "    hotels = soup.findAll(\"a\", {'class': 'restnt-name ellipsis'})\n",
    "    for hotel in hotels:\n",
    "        # print(hotel.get_text())\n",
    "        hotels_df.append(hotel.get_text())\n",
    "        \n",
    "        \n",
    "    # Extracting Cuisine\n",
    "    Cuisines_df = []\n",
    "    temp_df = []\n",
    "    # average_cost = []\n",
    "    Cuisines = soup.findAll(\"span\", class_=\"double-line-ellipsis\")\n",
    "    for Cuisine in Cuisines:\n",
    "        # print(Cuisine.get_text().split(\"|\"))\n",
    "        temp_df.append(Cuisine.get_text().split(\"|\"))\n",
    "    for i in temp_df:\n",
    "        # average_cost.append(i[0])\n",
    "        Cuisines_df.append(i[1])\n",
    "    # print(Cuisines_df)\n",
    "    \n",
    "\n",
    "    # Extracting Restaurant Location\n",
    "    locations_df = []\n",
    "    locations = soup.findAll(\"div\", {'class': 'restnt-loc ellipsis'})\n",
    "    for location in locations:\n",
    "        # print(location.get_text())\n",
    "        locations_df.append(location.get_text())\n",
    "        \n",
    "        \n",
    "    # Extracting Restaurant Ratings\n",
    "    Ratings_df = []\n",
    "    Ratings = soup.findAll(\"div\", {'class': 'restnt-rating rating-4'})\n",
    "    for Rating in Ratings:\n",
    "        # print(Rating.get_text())\n",
    "        Ratings_df.append(Rating.get_text())\n",
    "        \n",
    "\n",
    "    # Extracting Image Urls\n",
    "    image_links_df = []\n",
    "    image_links = soup.findAll(\"img\", {'class': 'no-img'})\n",
    "    for link in image_links:\n",
    "        # print(link[\"data-src\"])\n",
    "        image_links_df.append(link[\"data-src\"])\n",
    "        \n",
    "        \n",
    "    # Creating Data Frame\n",
    "    restaurants = pd.DataFrame()\n",
    "    restaurants[\"Restaurant name\"] = hotels_df\n",
    "    restaurants[\"Cuisine\"] = Cuisines_df\n",
    "    restaurants[\"Location\"] = locations_df\n",
    "    restaurants[\"Ratings\"] = Ratings_df\n",
    "    restaurants[\"Image Urls\"] = image_links_df\n",
    "    # print(image_links_df[0])\n",
    "    print(restaurants)\n",
    "    \n",
    "    \n",
    "restaurant_details(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1f650",
   "metadata": {},
   "source": [
    "# <font color='fuchsia'>10) Write a python program to scrape the details of top publications from Google Scholar.</font>\n",
    "\n",
    "## Link: https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "#### i) Rank\n",
    "#### ii) Publication\n",
    "#### iii) h5-index\n",
    "#### iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "062923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_publications(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    # Extracting Rank\n",
    "    ranks_df = []\n",
    "    ranks = soup.findAll(\"td\", {'class': 'gsc_mvt_p'})\n",
    "    for rank in ranks:\n",
    "        # print(rank.get_text()[:-1])\n",
    "        ranks_df.append(rank.get_text()[:-1])\n",
    "        \n",
    "        \n",
    "    # Extracting Publications\n",
    "    Publications_df = []\n",
    "    Publications = soup.findAll(\"td\", {'class': 'gsc_mvt_t'})\n",
    "    for publication in Publications:\n",
    "        # print(publication.get_text())\n",
    "        Publications_df.append(publication.get_text())\n",
    "        \n",
    "        \n",
    "    # Extracting h5_index and Median\n",
    "    indexes_medians_df = []\n",
    "    h5_indexes = soup.findAll(\"td\", {'class': 'gsc_mvt_n'})\n",
    "    for index in h5_indexes:\n",
    "        # print(index.get_text())\n",
    "        indexes_medians_df.append(index.get_text())\n",
    "    res = indexes_medians_df[::2] + indexes_medians_df[1::2]\n",
    "    h5_medians_df = res[(len(indexes_medians_df)//2):]\n",
    "    h5_indexes_df = res[:(len(indexes_medians_df)//2)]\n",
    "    \n",
    "    \n",
    "    # Converting to Data Frame\n",
    "    articles = pd.DataFrame()\n",
    "    articles[\"Rank\"] = ranks_df\n",
    "    articles[\"Publications\"] = Publications_df\n",
    "    articles[\"h5_Index\"] = h5_indexes_df\n",
    "    articles[\"h5_Median\"] = h5_medians_df\n",
    "\n",
    "    print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d450a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank                                       Publications h5_Index h5_Median\n",
      "0     1                                             Nature      444       667\n",
      "1     2                The New England Journal of Medicine      432       780\n",
      "2     3                                            Science      401       614\n",
      "3     4  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
      "4     5                                         The Lancet      354       635\n",
      "..  ...                                                ...      ...       ...\n",
      "95   96                       Journal of Business Research      145       233\n",
      "96   97                                   Molecular Cancer      145       209\n",
      "97   98                                            Sensors      145       201\n",
      "98   99                              Nature Climate Change      144       228\n",
      "99  100                    IEEE Internet of Things Journal      144       212\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "top_publications(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
